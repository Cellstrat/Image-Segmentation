{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Virtual Background.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaggernaut007/Image-Segmentation/blob/master/Virtual_Background.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0DzEoCE8CTs",
        "colab_type": "text"
      },
      "source": [
        "### This notebook is optionally accelerated with a GPU runtime.\n",
        "### If you would like to use this acceleration, please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "\n",
        "# Deeplabv3-ResNet101\n",
        "\n",
        "*Author: Pytorch Team and AI Mage team*\n",
        "\n",
        "(https://www.aimage.in)\n",
        "\n",
        "**DeepLabV3 model with a ResNet-101 backbone**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "volKZWew8CTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "model = torch.hub.load('pytorch/vision:v0.6.0', 'deeplabv3_resnet101', pretrained=True)\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XifmpnYY8CT0",
        "colab_type": "text"
      },
      "source": [
        "All pre-trained models expect input images normalized in the same way,\n",
        "i.e. mini-batches of 3-channel RGB images of shape `(N, 3, H, W)`, where `N` is the number of images, `H` and `W` are expected to be at least `224` pixels.\n",
        "The images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]`\n",
        "and `std = [0.229, 0.224, 0.225]`.\n",
        "\n",
        "The model returns an `OrderedDict` with two Tensors that are of the same height and width as the input Tensor, but with 21 classes.\n",
        "`output['out']` contains the semantic masks, and `output['aux']` contains the auxillary loss values per-pixel. In inference mode, `output['aux']` is not useful.\n",
        "So, `output['out']` is of shape `(N, 21, H, W)`. More documentation can be found [here](https://pytorch.org/docs/stable/torchvision/models.html#object-detection-instance-segmentation-and-person-keypoint-detection)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsx4BLBw8CT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download an example image from the pytorch website\n",
        "import urllib\n",
        "url, filename = (\"https://tvguide1.cbsistatic.com/i/r/2018/02/10/3a534e4d-ffa5-402d-b68d-cb0c7368e6af/thumbnail/1300x867/484fb3fb0611a3cbafb8dc62d26bdcd1/180209-personofinterest.jpg\", \"input.jpg\")\n",
        "try: urllib.URLopener().retrieve(url, filename)\n",
        "except: urllib.request.urlretrieve(url, filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG6GKd0h8GXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url2, filename2 = (\"https://i.ytimg.com/vi/cdWj8fXHQLg/maxresdefault.jpg\", \"bg.jpg\")\n",
        "try: urllib.URLopener().retrieve(url2, filename2)\n",
        "except: urllib.request.urlretrieve(url2, filename2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoGocnfjogkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "input_image = Image.open(filename)\n",
        "bgImage = Image.open(filename2)\n",
        "\n",
        "print(\"Initial image sizes\",input_image.size,bgImage.size)\n",
        "input_image =input_image.resize(bgImage.size)\n",
        "\n",
        "#resize background image if background image is bigger than the target image.\n",
        "#bgImage =bgImage.resize(input_image.size)\n",
        "print(\"Make sure these  values are the same: \",input_image.size,bgImage.size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1aE67A_8CT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample execution (requires torchvision)\n",
        "#todo Fix inference\n",
        "from torchvision import transforms\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "# move the input and model to GPU for speed if available\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)['out'][0]\n",
        "output_predictions = output.argmax(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyY3WMU68CUB",
        "colab_type": "text"
      },
      "source": [
        "The output here is of shape `(21, H, W)`, and at each location, there are unnormalized proababilities corresponding to the prediction of each class.\n",
        "To get the maximum prediction of each class, and then use it for a downstream task, you can do `output_predictions = output.argmax(0)`.\n",
        "\n",
        "Here's a small snippet that plots the predictions, with each color being assigned to each class (see the visualized image on the left)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJOsy0_r8CUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "#map person to index\n",
        "LABEL_NAMES = np.asarray([\n",
        "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
        "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
        "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n",
        "])\n",
        "\n",
        "# plot the semantic segmentation predictions of 21 classes in each color\n",
        "r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)\n",
        "\n",
        "# #resize for adjusting quality \n",
        "# size = 720, 720\n",
        "# r.thumbnail(size, Image.ANTIALIAS)\n",
        "# input_image.thumbnail(size, Image.ANTIALIAS)\n",
        "# bgImage.thumbnail(size, Image.ANTIALIAS)\n",
        "\n",
        "r_np = np.asarray(r)\n",
        "img_np = np.asarray(input_image)\n",
        "bgImage_np =np.array(bgImage)\n",
        "bgImage_np.setflags(write=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKMTRnWLp171",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(r_np.shape,img_np.shape,bgImage_np.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJR_J90dqhZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Super impose person on Bakground image\n",
        "for i in range(r_np.shape[0]):\n",
        "  for j in range(r_np.shape[1]):\n",
        "    if(r_np[i][j]==15):\n",
        "      bgImage_np[i][j] = img_np[i][j]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGSUksFqtEGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert Numpy to image\n",
        "virtualBG = Image.fromarray(bgImage_np)\n",
        "\n",
        "#Show Image\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(virtualBG)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqJpU8dk8CUJ",
        "colab_type": "text"
      },
      "source": [
        "### Model Description\n",
        "\n",
        "Deeplabv3-ResNet101 is contructed by a Deeplabv3 model with a ResNet-101 backbone.\n",
        "The pre-trained model has been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n",
        "\n",
        "Their accuracies of the pre-trained models evaluated on COCO val2017 dataset are listed below.\n",
        "\n",
        "|    Model structure  |   Mean IOU  | Global Pixelwise Accuracy |\n",
        "| ------------------- | ----------- | --------------------------|\n",
        "| deeplabv3_resnet101 |   67.4      |   92.4                    |\n",
        "\n",
        "### Resources\n",
        "\n",
        " - [Rethinking Atrous Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1706.05587)"
      ]
    }
  ]
}